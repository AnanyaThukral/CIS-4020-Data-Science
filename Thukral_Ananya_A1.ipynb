{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 4020\n",
    "# Assignment 1 - [Ananya Thukral] - [id]\n",
    "## Part 1 [20 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6 Marks] \n",
    "# a\n",
    "\n",
    "Write a simple implementation of a least-squares solution to linear regression that applies an iterative update to adjust the weights. Demonstrate the success of your approach on the sample data loaded below, and visualize the best fit plotted as a line (consider using linspace) against a scatter plot of the x and y test values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = diabetes_X_train\n",
    "y = diabetes_y_train\n",
    "\n",
    "# m denotes the number of examples here, not the number of features\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta\n",
    "\n",
    "\n",
    "def genData(numPoints, bias, variance):\n",
    "    x = np.zeros(shape=(numPoints, 2))\n",
    "    y = np.zeros(shape=numPoints)\n",
    "    # basically a straight line\n",
    "    for i in range(0, numPoints):\n",
    "        # bias feature\n",
    "        x[i][0] = 1\n",
    "        x[i][1] = i\n",
    "        # our target variable\n",
    "        y[i] = (i + bias) + random.uniform(0, 1) * variance\n",
    "    return x, y\n",
    "\n",
    "# gen 100 points with a bias of 25 and 10 variance as a bit of noise\n",
    "x, y = genData(100, 25, 10)\n",
    "m, n = np.shape(x)\n",
    "numIterations= 100000\n",
    "alpha = 0.0005\n",
    "theta = np.ones(n)\n",
    "theta = gradientDescent(x, y, theta, alpha, m, numIterations)\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 Marks]\n",
    "# b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data on movie ratings, revenue, metadata etc. Split data into a relevant set for training, testing and classification. Explain your choice of split. It is ok if you decide to split into these subsets after part c -> if you do so, mention this at the end of your explanation.\n",
    "\n",
    "Explanation: \n",
    "\n",
    "- My part b,c,d are all together.\n",
    "- All the code below has comments and explanations.\n",
    "- Please, note that if you encounter any errors it might be because either you're running the cells in incorrect order or your anaconda doesn't have required packages installed.\n",
    "- First cell below starts with data loading, cleaning and picking independent variables\n",
    "- Starting from next call I have performed done Data Exploratory Analysis to understand what type of data I am dealing with and if it consists of any interesting patters, trends and most importantly how the columns/varaibles of data are correlated with each other. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### An example to load a csv file\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "meta_data=pd.read_csv('movies_metadata.csv', low_memory=False) # You may wish to specify types, or process columns once read\n",
    "ratings_small=pd.read_csv('ratings_small.csv')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "##### YOUR CODE HERE #######\n",
    "\n",
    "#load other files\n",
    "keywords = pd.read_csv('keywords.csv', low_memory=False)\n",
    "# credits = pd.read_csv('credits.csv', low_memory=False)\n",
    "\n",
    "#change, clean and remove\n",
    "meta_data['id'] = pd.to_numeric(meta_data['id'],errors='coerce',downcast='integer')\n",
    "meta_data['budget'] = pd.to_numeric(meta_data['budget'],errors='coerce',downcast='integer')\n",
    "meta_data[['id', 'budget']] = meta_data[['id','budget']].fillna(value=0)\n",
    "meta_data = meta_data.astype({'id': 'Int64', 'budget': 'Int64'})\n",
    "meta_data.drop(meta_data[meta_data['budget']<=10000].index,inplace = True)\n",
    "meta_data.drop(meta_data[meta_data['revenue']<=1000].index,inplace = True)\n",
    "meta_data['runtime'].replace('', np.nan, inplace=True)\n",
    "meta_data.dropna(subset=['runtime'], inplace=True)\n",
    "\n",
    "#Left Merge/Join the data using id\n",
    "# dataset = meta_data.merge(ratings_small,left_on='id',right_on='movieId',how='left')\n",
    "dataset = meta_data.merge(keywords,left_on='id',right_on='id',how='left')\n",
    "# dataset = dataset.merge(credits,left_on='id',right_on='id',how='left')\n",
    "\n",
    "#more data cleaning\n",
    "dataset['genres'] = dataset['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else[])\n",
    "dataset['spoken_languages'] = dataset['spoken_languages'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else[])\n",
    "dataset['production_companies'] = dataset['production_companies'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else[])    \n",
    "dataset['keywords'] = dataset['keywords'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else[])\n",
    "dataset['year'] = pd.to_datetime(dataset['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)\n",
    "dataset['month'] = pd.to_datetime(dataset['release_date'], errors='coerce').apply(lambda x: str(x.month) if x != np.nan else np.nan)\n",
    "dataset['day'] = pd.to_datetime(dataset['release_date'], errors='coerce').apply(lambda x: str(x.day) if x != np.nan else np.nan)\n",
    "\n",
    "X = dataset.iloc[:, :].values\n",
    "y_revenue = dataset.iloc[:, 15].values\n",
    "\n",
    "#picking independent variables (feature set - mostly numeric - easy to train/test data)\n",
    "X = X[:,[2,16,23,25,26,27]]\n",
    "\n",
    "# Removing zero REVENUES from the data - revenue is super important\n",
    "# I could (and have) adjusted for inflation, but it made scant difference to model performance\n",
    "y_revenue_removed = []\n",
    "X_removed = []\n",
    "for l in range(0,len(y_revenue)):\n",
    "    if y_revenue[l] !=0:\n",
    "        y_revenue_removed.append(y_revenue[l])\n",
    "        X_removed.append(X[l])\n",
    "y_revenue = np.array(y_revenue_removed)\n",
    "X = np.array(X_removed)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relation of Revenue with other attributes/columns\n",
    "import seaborn as sns\n",
    "sns.heatmap(dataset.corr(), cmap='YlGnBu', annot=True, linewidths = 0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating log transformation for reveune\n",
    "dataset['log_revenue'] = np.log1p(dataset['revenue']) #we are not using log0 to avoid & and null value as there might be 0 value\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# seting the distribuition of our data and normalizing using np.log on values highest than 0 and + \n",
    "# also, we will set the number of bins and if we want or not kde on our histogram\n",
    "ax = sns.distplot(dataset['log_revenue'])\n",
    "ax.set_xlabel('Revenue', fontsize=15) #seting the xlabel and size of font\n",
    "ax.set_ylabel('Distribuition', fontsize=15) #seting the ylabel and size of font\n",
    "ax.set_title(\"Distribuition of Revenue(log transformation)\", fontsize=20) #seting the title and size of font\n",
    "\n",
    "#comapring distribution of reveune and log revune side by side with histogram\n",
    "fig, ax = plt.subplots(figsize = (16, 6))\n",
    "plt.subplot(1, 2, 1) #1 means 1 plot, 2 means column and 1 mean 1 sub plot\n",
    "plt.hist(dataset['revenue']);\n",
    "plt.title('Distribution of revenue');\n",
    "plt.subplot(1, 2, 2)#1 means 1 plot, 2 means column and 2 mean second sub plot\n",
    "plt.hist(dataset['log_revenue']);\n",
    "plt.title('Distribution of log transformation of revenue');\n",
    "\n",
    "#Relationship between Revenue and Budget\n",
    "dataset['log_budget'] = np.log1p(dataset['budget'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(dataset['budget']);\n",
    "plt.title('Distribution of Budget');\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(dataset['log_budget']);\n",
    "plt.title('Distribution of log transformation of budget');\n",
    "\n",
    "#let's create scatter plot (Revenue and Budget)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(dataset['budget'], dataset['revenue'])\n",
    "plt.title('Revenue vs Budget fig(1)');\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(dataset['log_budget'], dataset['log_revenue'])\n",
    "plt.title('Log Revenue vs Log Budget fig(2)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: professor said that we are allowed to either sketch the scatter plot for revenue vs rating or reveneue vs vote_count. Therefore, please consider the below picture (revenue vs vote_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transformation of vote count\n",
    "dataset['log_vote_count'] = np.log1p(dataset['vote_count'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(dataset['vote_count']);\n",
    "plt.title('Distribution of vote count');\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(dataset['log_vote_count']);\n",
    "plt.title('Distribution of log transformation of vote count');\n",
    "\n",
    "#let's create scatter plot (Revenue and Vote count)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(dataset['vote_count'], dataset['revenue'])\n",
    "plt.title('Revenue vs vote count fig(1)');\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(dataset['log_vote_count'], dataset['log_revenue'])\n",
    "plt.title('Log Revenue vs log vote count fig(2)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's find top words from movie Titles\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "start = time.time()\n",
    "plt.figure(figsize = (12, 12))\n",
    "token_title = ' '.join(dataset['original_title'].values) #create split to title by sprace to extract the text.\n",
    "#bg color set to white for good contrast, by default bg color is darker\n",
    "wordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(token_title)\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Top words from movie titles ')\n",
    "plt.axis(\"off\") # we dont need axes for this\n",
    "plt.show()\n",
    "print(\" Time taken to complete this operation is\", time.time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between Release Date and Revenue\n",
    "from matplotlib import pyplot as plt \n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "dataset = dataset.dropna() \n",
    "\n",
    "# Count no.of films released per year and sort the years in ascending order\n",
    "# Do this for both Train and Test Sets\n",
    "d1 = dataset['year'].value_counts().sort_index()\n",
    "\n",
    "# x values are years, and y values are movie counts, name=legend\n",
    "data = go.Scatter(x=d1.index, y=d1.values, name='movies data')\n",
    "\n",
    "layout = go.Layout(title = \"Number of films per year\", xaxis_title = 'Release date in Year',yaxis_title = 'Movie Count')\n",
    "py.iplot(dict(data=data, layout=layout))\n",
    "\n",
    "#countplot chart for movies release year\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.countplot(dataset['year'].sort_values())\n",
    "plt.title(\"Movie Release count by Year\",fontsize=20)\n",
    "loc, labels = plt.xticks()\n",
    "plt.xticks(fontsize=12,rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue and Month of Movie Release\n",
    "plt.figure(figsize=(20,5));\n",
    "\n",
    "sns.catplot(x='month', y='revenue', data=dataset);\n",
    "plt.title('Revenue on month of movie release');\n",
    "#lets replace number by actual month name\n",
    "loc, labels = plt.xticks()\n",
    "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "plt.xticks(loc, labels,fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue and Genres\n",
    "# Note: if you run this after last cell it won't work because you'll be deleting the genres from the main dataset. In that \n",
    "# case, you can go and run the first cell again :).\n",
    "\n",
    "dataset = dataset.dropna() \n",
    "unique_genres = dataset[\"genres\"].explode().unique() \n",
    "genres_dummies = pd.get_dummies(dataset[\"genres\"].apply(pd.Series).stack()).sum(level=0) #one hot encoding\n",
    "train_genres = pd.concat([dataset, genres_dummies],axis=1, sort=False)\n",
    "genres_overall = train_genres[unique_genres].sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(15,5))\n",
    "ax = sns.barplot(x=genres_overall.index, y=genres_overall.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Popularity of genres overall\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Run cells in proper order. Thank you :)\n",
    "#Revenue vs Runtime\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(dataset['runtime'].fillna(0) / 60, bins=40, kde=False); #filling runtime with 0 if there were any missing values\n",
    "plt.title('Distribution of length of film in hours');\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(dataset['runtime'].fillna(0)/60, dataset['revenue'])\n",
    "plt.title('runtime vs revenue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conver the cleaned data into dataframe and save them to csv\n",
    "\n",
    "dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "dataset.fillna(0, inplace=True)\n",
    "\n",
    "#rename the columns in the dataset for feature variables/set\n",
    "dataset =  pd.DataFrame(data = X,  columns = ['budget','runtime', 'vote_count','year','month','day'])\n",
    "\n",
    "#saving to CSVs as a checkpoint to be used in regressors\n",
    "dataset.to_csv(r'Clean_Data.csv')\n",
    "dataset_y_revenue = pd.DataFrame(y_revenue, columns = ['revenue'])\n",
    "dataset_y_revenue.to_csv(r'Encoded_y - revenue.csv')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Marks]\n",
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the data into relevant features for predicting revenue. <br>\n",
    "\n",
    "i.  Explain your feature sets and organization. <br>\n",
    "\n",
    "YOUR EXPLANATION HERE (feature sets - input variables)\n",
    "\n",
    "Feature Set:\n",
    "\n",
    " - Performed the Data Exploratory Analysis - Plotted the correlation between revenue and other attributes in the movies data.\n",
    " - Correlation Plot - Four variables selected (Vote Count, Budget, Runtime and Release Date - split into Year, Month, Day). \n",
    " - Firstly, they have the most significant relationship with revenue (check the correlation plot and above analysis). \n",
    " - Revenue is correlated with vote count, budget, runtime with a value of 0.77, 0.73, 0.19 respectively.\n",
    " - Secondly, they are numeric values. Hence, it becomes a lot easier to train and test the data.\n",
    " \n",
    "Data Cleaning Process:\n",
    "\n",
    "  - Removed Revenue and Budget rows that had 0/nan value. Further, adjusted revenue for inflation. \n",
    "  - Dropped rows from the metadata that had nan's to make it more clean and accurate for prediction. That resulted in lot of rows being lost but is better than bad data. \n",
    "  - Considered Budget value >= 10,000 and Revenue >1000. This data is not good for analysis purposes and wouldn't help much in Revenue prediction.\n",
    "  - Columns - \"Genre\", \"keywords\", \"production company\", \"spoken languages\" JSON data transformed to List for exploratory analysis.\n",
    "  - \"Release Date\" - As month, year is categorial data and not continous data. Therefore, i created catplot to explore the data (check the plots above). Further, the \"Release Date\" was split into Year, month and Day for Analysis. As a result from the catplot - There seems to have some correlation but it may not have one to one casual effect.\n",
    "\n",
    "\n",
    "ii. Plot movie revenue vs. rating as a scatter plot and discuss your findings. <br>\n",
    "\n",
    "- Already plotted above. I chose to draw a scatter plot for revenue vs vote_count\n",
    "\n",
    "iii. Visualize any other relationships you deem interesting and explain. <br>\n",
    "\n",
    "\n",
    "- Revenue and Budget: One can build a very good prediction of a film’s revenue based purely on inputs known before the film goes public. This has real world consequences: for instance, a cinema could use this to predict how long they’d like to run a film for, ahead of time. The plot above reflects the trend that with more budget we see increased revenue. \n",
    "\n",
    "- Genres: Action is the most popular Genre, followed by Adventure and Thriller\n",
    "\n",
    "- We can see that since 2000s there are more movies that has been released. Release Date: Years between 2011-2012 has the most movies released. \n",
    "\n",
    "- From the above chart we can see also see that movie released in November has maximum revenue where as movie released in April has less revenue compared to other months.\n",
    "\n",
    "- Revenue and Run time - Here we have run time in hour on x-axis and freq of movie in on y axis and then we cas see that most of the movie are between 1-3 hr. Further, the movie that fall on this duration has highest revenue.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3 Marks]\n",
    "# d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regression model to predict movie revenue. Plot predicted revenue vs. actual revenue on the test set. Quantify the error in your prediction. (You may use sklearn for this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regression model here, plot your fit to the revenue data versus the actual data from the test set as a scatter plot.\n",
    "\n",
    "##### YOUR CODE HERE #######\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dataset_X_reimported = pd.read_csv('Clean_Data.csv') #clean data\n",
    "dataset_y_reimported = pd.read_csv('Encoded_y - revenue.csv') #data is clean with no 0 revenue\n",
    "dataset_reimported = pd.concat([dataset_X_reimported,dataset_y_reimported],axis=1)\n",
    "dataset_reimported = dataset_reimported.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X = dataset_reimported.iloc[:, 1:-2].values\n",
    "y = dataset_reimported.iloc[:, -1].values\n",
    "\n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#I have a fairly large dataset of +- 4000 entries, so I'm going with 10% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "#linear regression\n",
    "lm = LinearRegression() #our 6th model\n",
    "lm.fit(X_train, y_train)\n",
    "lm_preds = lm.predict(X_test)\n",
    "print(\"Linea Regression, R Square: \", r2_score(y_test, lm_preds))\n",
    "\n",
    "#training score\n",
    "training_score = lm.score(X_train, y_train)\n",
    "print(\"Training score: \", score)\n",
    "\n",
    "#mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred) \n",
    "print(\"Linea Regression MSE: %.2f\" % mse)\n",
    "\n",
    "#Visualization 1\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_pred)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured revenue')\n",
    "ax.set_ylabel('Predicted revenue')\n",
    "plt.title('Measured versus predicted revenue')\n",
    "plt.ylim((50000000, 300000000))   # set the ylim to bottom, top\n",
    "plt.xlim(50000000, 300000000)     # set the ylim to bottom, top\n",
    "plt.show()\n",
    "\n",
    "#Visualization 2\n",
    "x_ax = range(len(y_test))\n",
    "plt.plot(x_ax, y_test, label=\"original\")\n",
    "plt.plot(x_ax, y_pred, label=\"predicted\")\n",
    "plt.title(\"Actual Revenue and Predicted Revenue\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBRegressor  \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "regressor = XGBRegressor(colsample_bytree= 0.6, gamma= 0.7, max_depth= 4, min_child_weight= 5,\n",
    "                         subsample = 0.8, objective='reg:squarederror')\n",
    "\n",
    "predictions2 = regressor.fit(X, y)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "#training score\n",
    "training_score = regressor.score(X_train, y_train)\n",
    "print(\"Training score: \", score)\n",
    "\n",
    "#mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred) \n",
    "print(\"MSE: %.2f\" % mse)\n",
    "\n",
    "#R2 score\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"R square : \",score)\n",
    "\n",
    "#Predictions for the test data\n",
    "revenue_predictions = regressor.predict(X_test)\n",
    "gbr_predictions = pd.DataFrame(revenue_predictions, columns = ['predicted_revenue'])\n",
    "gbr_predictions.head()\n",
    "\n",
    "test_result = pd.concat([dataset_reimported, gbr_predictions], axis = 1, sort=True)\n",
    "test_result = test_result[[ 'budget','year','month', 'revenue','predicted_revenue']]\n",
    "print(test_result.head())\n",
    "\n",
    "#Visualization 1\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_pred)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured revenue')\n",
    "ax.set_ylabel('Predicted revenue')\n",
    "plt.title('Measured versus predicted revenue')\n",
    "plt.ylim((50000000, 300000000))   # set the ylim to bottom, top\n",
    "plt.xlim(50000000, 300000000)     # set the ylim to bottom, top\n",
    "plt.show()\n",
    "\n",
    "#Visualization 2\n",
    "x_ax = range(len(y_test))\n",
    "plt.plot(x_ax, y_test, label=\"original\")\n",
    "plt.plot(x_ax, y_pred, label=\"predicted\")\n",
    "plt.title(\"Actual Revenue and Predicted Revenue\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4 Marks]\n",
    "# e\n",
    "\n",
    "Try a non-linear fit to the data, with and without regularization. Find your best fit and justify the choice of parameters, regularization constant and norm. Plot predicted revenue vs. actual revenue on the test set. In each case, quantify the error. (See e.g. Generalized linear models, Kernel Ridge regression, SVR and others from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE WITHOUT REGULARIZATION #######\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "dataset_X_reimported = pd.read_csv('Clean_Data.csv') #clean data\n",
    "dataset_y_reimported = pd.read_csv('Encoded_y - revenue.csv') #data is clean with no 0 revenue\n",
    "dataset_reimported = pd.concat([dataset_X_reimported,dataset_y_reimported],axis=1)\n",
    "dataset_reimported = dataset_reimported.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X = dataset_reimported.iloc[:, 1:-2]\n",
    "y = dataset_reimported.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "#without regularization, alpha = 0.0\n",
    "krr = KernelRidge(alpha=0.0) #regularization parameter\n",
    "krr.fit(X_train, y_train)\n",
    "pred = krr.predict(X_test)\n",
    "\n",
    "#Mean Square Error\n",
    "MSE = mean_squared_error(y_test, pred)\n",
    "print('Mean Squared Error: ', MSE)\n",
    "      \n",
    "#R2 score\n",
    "score = r2_score(y_test, pred)\n",
    "print(\"R square : \",score)\n",
    "\n",
    "#Visualization 1 \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, pred)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured revenue')\n",
    "ax.set_ylabel('Predicted revenue')\n",
    "plt.title('Measured versus predicted revenue')\n",
    "plt.ylim((50000000, 300000000))   # set the ylim to bottom, top\n",
    "plt.xlim(50000000, 300000000)     # set the ylim to bottom, top\n",
    "plt.show()\n",
    "\n",
    "# Subplot top-left\n",
    "x_ax = range(len(y_test))\n",
    "plt.plot(x_ax, y_test, label=\"original\")\n",
    "plt.plot(x_ax, pred, label=\"predicted\")\n",
    "plt.title(\"Actual Revenue and Predicted Revenue\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE WITH REGULARIZATION #######\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dataset_X_reimported = pd.read_csv('Clean_Data.csv') #clean data\n",
    "dataset_y_reimported = pd.read_csv('Encoded_y - revenue.csv') #data is clean with no 0 revenue\n",
    "dataset_reimported = pd.concat([dataset_X_reimported,dataset_y_reimported],axis=1)\n",
    "dataset_reimported = dataset_reimported.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X = dataset_reimported.iloc[:, 1:-2]\n",
    "y = dataset_reimported.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "#with regularization\n",
    "krr = KernelRidge(alpha=2.9) #regularization parameter (for example alpha = 2.9)\n",
    "krr.fit(X_train, y_train)\n",
    "pred = krr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "print('For alpha 2.9, MSE error: ', mse)\n",
    "#R2 score\n",
    "score = r2_score(y_test, pred)\n",
    "print(\"For alpha 2.9 R square : \",score)\n",
    "\n",
    "#Predictions for the test data\n",
    "revenue_predictions = krr.predict(X_test)\n",
    "gbr_predictions = pd.DataFrame(revenue_predictions, columns = ['predicted_revenue'])\n",
    "gbr_predictions.head()\n",
    "\n",
    "test_result = pd.concat([dataset_reimported, gbr_predictions], axis = 1, sort=True)\n",
    "test_result = test_result[[ 'budget','year','month', 'revenue','predicted_revenue']]\n",
    "print(test_result.head())\n",
    "\n",
    "#Visualization 1 \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, pred)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured revenue')\n",
    "ax.set_ylabel('Predicted revenue')\n",
    "plt.title('Measured versus predicted revenue')\n",
    "plt.ylim((50000000, 300000000))   # set the ylim to bottom, top\n",
    "plt.xlim(50000000, 300000000)     # set the ylim to bottom, top\n",
    "plt.show()\n",
    "\n",
    "# Subplot top-left\n",
    "x_ax = range(len(y_test))\n",
    "plt.plot(x_ax, y_test, label=\"original\")\n",
    "plt.plot(x_ax, pred, label=\"predicted\")\n",
    "plt.title(\"Actual Revenue and Predicted Revenue\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 [10 Marks]\n",
    "\n",
    "[4 Marks]\n",
    "# a\n",
    "\n",
    "Write a simple version of the basic algorithm for k-means clustering. Simple here means the core of the algorithm and not optimizations or extensions you might find in standard python libraries. Typically you might rely on a standard library for doing this, but it helps to see the core by manipulating the data and labels by hand as practice for numerical python and how to frame the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# Generate Samples\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "\n",
    "m=X.shape[0]\n",
    "n=X.shape[1]\n",
    "n_iter=50\n",
    "K=4\n",
    "\n",
    "#creating an empty centeroid array\n",
    "centeroids = np.array([]).reshape(n,0)\n",
    "\n",
    "#creating 4 random centeroids\n",
    "for k in range(K):\n",
    "    centeroids = np.c_[centeroids,X[random.randint(0,m-1)]]\n",
    "\n",
    "\n",
    "#distance formula between points (Euclidean distance)\n",
    "euclid = np.array([]).reshape(m,0)\n",
    "\n",
    "#finding distance between each centeroid\n",
    "for k in range(K):\n",
    "    dist=np.sum((X-centeroids[:,k])**2,axis=1)\n",
    "    euclid=np.c_[euclid,dist]\n",
    "    \n",
    "# storing the minimum value we have computed\n",
    "minimum=np.argmin(euclid,axis=1)+1\n",
    "\n",
    "# computing the mean of separated clusters\n",
    "cent={}\n",
    "for k in range(K):\n",
    "    cent[k+1]=np.array([]).reshape(2,0)\n",
    "\n",
    "# assigning of clusters to points\n",
    "for k in range(m):\n",
    "    cent[minimum[k]]=np.c_[cent[minimum[k]],X[k]]\n",
    "for k in range(K):\n",
    "    cent[k+1]=cent[k+1].T\n",
    "\n",
    "# computing mean and updating it\n",
    "for k in range(K):\n",
    "     centeroids[:,k]=np.mean(cent[k+1],axis=0)\n",
    "        \n",
    "# repeating the above steps again and again\n",
    "for i in range(n_iter):\n",
    "    euclid=np.array([]).reshape(m,0)\n",
    "    for k in range(K):\n",
    "        dist=np.sum((X-centeroids[:,k])**2,axis=1)\n",
    "        euclid=np.c_[euclid,dist]\n",
    "    C=np.argmin(euclid,axis=1)+1\n",
    "    cent={}\n",
    "    for k in range(K):\n",
    "        cent[k+1]=np.array([]).reshape(2,0)\n",
    "    for k in range(m):\n",
    "        cent[C[k]]=np.c_[cent[C[k]],X[k]]\n",
    "    for k in range(K):\n",
    "        cent[k+1]=cent[k+1].T\n",
    "    for k in range(K):\n",
    "        centeroids[:,k]=np.mean(cent[k+1],axis=0)\n",
    "    final=cent\n",
    "    \n",
    "\n",
    "for k in range(K):\n",
    "    plt.scatter(final[k+1][:,0],final[k+1][:,1])\n",
    "plt.scatter(centeroids[0,:],centeroids[1,:],s=200,c='black', alpha=0.5)\n",
    "plt.rcParams.update({'figure.figsize':(10,7.5), 'figure.dpi':100})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data set\"></a>\n",
    "[6 Marks]\n",
    "# b\n",
    "\n",
    "Load the mystery data below, and cluster the data (you don't need to use your code from part a). Visualize the data including assigned cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the mystery data here and cluster using k-means (now you can use libraries e.g. sklearn)\n",
    "mystery = np.load('mystery.npy')\n",
    "mystery.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=np.load('mystery.npy')\n",
    "\n",
    "wx = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, random_state = 0)\n",
    "    kmeans.fit(X)\n",
    "    wx.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wx)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "kmeans=KMeans(n_clusters=n, random_state=20).fit(X)\n",
    "labels_of_clusters = kmeans.fit_predict(X)\n",
    "print(labels_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=30309, centers=5,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(X)\n",
    "cc=kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cc, s=10, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters = Possibly between 5-7\n",
    "\n",
    "YOUR EXPLANATION HERE\n",
    "\n",
    "(any additional code supporting your assertion on the number of clusters may be included below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR (OPTIONAL) CODE HERE #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 Marks]\n",
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mystery data in part 2? Show this in markdown and code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLANATION HERE, code goes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR (OPTIONAL) CODE HERE #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10 Marks]\n",
    "# Challenge question (required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe how you might implement a solution to recommend new movies to a user based on their existing preferences or ratings from Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR EXPLANATION HERE, provide an example for one user id below.\n",
    "\n",
    "- I will go with Collabrative Filtering technique that can filter out items that a user might like on the basis of ratings by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user. It looks at the items they like and combines them to create a ranked list of suggestions.\n",
    "\n",
    "- The data taken for example smaller set can be considered in a form of a matrix consisting of the ratings given by a set of users to some movies from a set of movies. Each row would contain the ratings given by a user, and each column would contain the ratings received by a movie.  \n",
    "\n",
    "- It is calculated only on the basis of the rating (explicit or implicit) a user gives to an item. For example, two users can be considered similar if they give the same ratings to ten movies despite there being a big difference in their age.\n",
    "\n",
    "- There is quite a few libraries in Python that can build a recommender but the one i ended up choosing is \"Surprise\". Surprise is a Python SciKit that comes with various recommender algorithms and similarity metrics to make it easy to build and analyze recommenders. \n",
    "\n",
    "- The Dataset module is used to load data from files, Pandas dataframes, or even built-in datasets available for experimentation. (MovieLens 100k is one of the built-in datasets in Surprise.)\n",
    "\n",
    "- Insallation done using: $ conda install -c conda-forge scikit-surprise\n",
    "\n",
    "- In the first step, the data is stored in a dictionary that is loaded into a Pandas dataframe and then into a Dataset object from Surprise. \n",
    "\n",
    "- In the next step, I used an Algorithm based on K-Nearest (k-NN). It is available in Surprise as KNNWithMeans. The recommender function is configured to use the cosine similarity and to find similar items using the item-based approach. \n",
    "\n",
    "- To make this recommender, I created a Trainset from data. Trainset is built using the same data (coming from ratings.csv) but contains more information about the data, such as the number of users and movies (n_movies, n_users) that are used by the algorithm. We can create it either by using the entire data or a part of the data. We can also divide the data into folds where some of the data will be used for training and some for testing. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE #######\n",
    "import pandas as pd\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNWithMeans\n",
    "\n",
    "\n",
    "#load data\n",
    "ratings_small=pd.read_csv('ratings_small.csv')\n",
    "ratings_small.columns\n",
    "\n",
    "df = pd.DataFrame(ratings_small)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df[[\"userId\", \"movieId\", \"rating\"]], reader)\n",
    "\n",
    "# Loads the builtin Movielens-100k data\n",
    "movielens = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# recommender Algorithm\n",
    "# To use movie-based cosine similarity\n",
    "\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  # Compute  similarities between movies\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "\n",
    "#Prediction - how a user with id = 1 may rate moviei d = 260\n",
    "trainingSet = data.build_full_trainset()\n",
    "\n",
    "algo.fit(trainingSet)\n",
    "\n",
    "prediction = algo.predict('1', 260)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The user 1 (Userid), would rate 260 (MovieId) as: \", prediction.est)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
